---
title: "Introduction to Health Association Analysis for Movement Behaviour Data"
output: 
  rmarkdown::html_vignette:
    number_sections: true
vignette: >
  %\VignetteIndexEntry{Introduction to Health Association Analysis for Movement Behaviour Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r,  include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
knitr::opts_chunk$set(fig.width=12, fig.height=8)
library(rmarkdown)
library(knitr)
devtools::load_all()
```

In this tutorial, we will go through a basic health association analysis using movement behaviour data.

As well as external packages, we'll use some helper functions we've prepared previously. If you want to modify them, you can find them in the file `R/utils.R` on the GitHub page (see also intro notes).

#  Load in data
First we will load required packages, and the data we already prepared.

```{r load-packages}
# First we need to install packages that aren't already present. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(devtools, ggplot2, reshape2, data.table)

# If you've installed this as a package you can load helper functions like this
library(week2DataChallenge)

# We then load the data prepared in the last tutorial.
# To do this with example data, replace the file location below with 
# "/cdtshared/wearables/health_data_files/dataset-with-preprocessing-done.csv" 
dAll <- data.frame(fread("../data_and_data_prep/dataset-with-preprocessing-done.csv"))
```

Note that this dataset has had some additional preprocessing (like that described at the end of the last tutorial) of the variables we're going to use e.g. collapsing classes and removing missing data. Please make sure you've done this for the ones you're using!!

We need to make sure some variables are factors, as subsequent code relies on that. If you change the variables you use, you might need to add some variables here. I also added in variables scaling the movement behaviour variables to the number of hours or minutes in a day: 
```{r final-prep}
dAll$sex <- as.factor(dAll$Sex)
dAll$broadAgeGroup <- as.factor(dAll$broadAgeGroup)

behaviour_vars <- c("MVPA", "LIPA", "SB", "sleep")
for (variable in behaviour_vars){
  dAll[, paste0(variable, "_min_per_day")] <- 24*60*dAll[, variable]
  dAll[, paste0(variable, "_hr_per_day")] <- 24*dAll[, variable]
}

```


#  Describe and explore the data
## Tables to describe the data
The following code constructs a table to describe accelerometer measured attributes by conventional groupings (e.g. age, sex, self-rated health, season). 

The code might look excessively complicated, because we wanted to save it as a html output. You could do the same thing using R data frames, and then work out formatting afterwards... 


```{r table}
# specify groups (i.e. major row blocks)
groups <-  c( "sex", "broadAgeGroup") # Fill this in with groups you want to look at

# specify accelerometer outcomes (i.e. column headings)
outcomeCols <- c( "acc.overall.avg", "LIPA_hr_per_day") # Fill this in with a list of outcomes you want to look at


# specify output file name
outHTML<-'table1.html'

# write table header code
html <- '<html><head><title>Acc summary table</title></head><table>'
html <- paste(html, '<tr><th></th>')
html <- paste(html, '<th>Individuals</th>')
for(col  in outcomeCols){
    html<-paste(html, '<th>', col, '</th>')
}
html <- paste(html, '</tr>')
html <- paste(html, '<tr><th></th>')
html <- paste(html, '<th>[ n ]</th>')
html <- paste(html, '<th>[mean &plusmn stdev m<i>g</i>]</th>')
html <- paste(html, '<th>[mean &plusmn stdev LIPA hr/dy]</th>')

# iterate through each major row block
for(group in groups){
    html<-paste(html, '\n<tr><td><b>', group, '</b></td><td></td><td></td><td>')
    html<-paste(html, '</td><td></td><td></td><td></td><td></td><td></td><td></td></tr>')
    group.tmp <- data.frame(matrix(ncol=4, nrow=0)) #df to store outputs for subsequent effect size calculations
    colnames(group.tmp) <- c("item", "outcome", "avg", "stdev")
    # report mean/stdev/n summary stats for each "item" in group e.g. each age category
    for(item in levels(dAll[[group]])){
        html<-paste(html, '<tr>')
        html<-paste(html, '<td>', item, '</td>')
        # temp store of all relevant outcome data related to this "item"
        itemData <- dAll[dAll[[group]]==item, outcomeCols]
        n <- nrow(itemData)
        html <- paste(html, '<td align="center">', format(n,big.mark=","), '</td>')

        # iterate through each accelerometer outcome (i.e. each column)
        for(outcome in outcomeCols){
            html <- paste(html, summary4tableHTML_simple(itemData[[outcome]],1))
            avg <- mean(itemData[[outcome]], na.rm=TRUE)
            stdev <- sd(itemData[[outcome]], na.rm=TRUE)
            n <- nrow(itemData)
            group.tmp <- rbind(group.tmp, data.frame(item= item, outcome=outcome,
                                     avg=avg, stdev=stdev, n = n))
        }
        html<-paste(html, '</tr>')
    }


# now calculate ANOVA p-value between subgroup "items" for each acc outcome
html<-paste(html, '<tr><td>p value<sup>A</sup></td><td></td>')
for(outcome in outcomeCols){
  formulaStr <- paste(outcome,' ~ ', group)
  itemANOVA <- aov(as.formula(formulaStr), data=dAll)
  
  # extract p-value for specific variable of interest
  tmp <- as.data.frame(summary(itemANOVA)[[1]])
  groupResult <- tmp[grep(group, rownames(tmp)), "Pr(>F)"]
  
  # write formatted p-value to html
  html <- paste(html, pValueHTML_cell(groupResult,1e-300))        
 }
 html <- paste(html, '</tr>')
 
 # calculate effect size between subgroup "items" for each acc outcome
 html <- paste(html, "<tr><td>Cohen's d</td><td></td>")
 for(outcome in outcomeCols){
   # read relevant subgroup mean/stdev values stored from summary stats derived earlier
  grouping <- group.tmp[(group.tmp[, "outcome"] ==outcome),]
  minVal <- min(grouping['avg'])
  maxVal <- max(grouping['avg'])
  lowest <- grouping[(grouping$avg==minVal), c('avg','stdev', 'n')]
  highest <- grouping[(grouping$avg==maxVal), c('avg','stdev', 'n')]
  print(grouping)
  
  # calculate Cohen's d effect size and format for HTML output
  cohenD <- effectSize(lowest$avg,lowest$stdev, lowest$n, highest$avg,highest$stdev, highest$n)
  cohenD <- format(round(cohenD, 2), nsmall=2)
  cohenD <- sub('^(-)?0[.]', '\\1.', cohenD)
  html <- paste(html, '<td align="center">', cohenD, '</td>')
 }

  html <- paste(html, '</tr>')

}
#finalise HTML table
html <- paste(html, '\n</table>')
html <- paste(html, '<br><i><sup>A</sup> Age, sex:</i> ')
html <- paste(html, 'One-way analysis of variance test used to compare metrics between groups')
html <- paste(html, '</html>')
# and write html to file
write(html, file = outHTML)

print(paste0('standard table done, written to: ', outHTML))
knitr::include_url(outHTML)
```

You might think more about the distribution of some of the accelerometry-derived variables - for some, this may not be the best way to characterise them (for example, the distribution of MVPA is very skewed). But it's good for a first description!

## Plots to describe the data
We'll also plot some of the variables to get a feel for how they're distributed.

```{r}
# overall physical activity
hist(dAll$acc.overall.avg, breaks=1000, xlim=c(0,100))

#look at deciles
quantile(dAll$acc.overall.avg, prob = seq(0.1, 0.9, by = 0.1), na.rm = TRUE)

# MVPA
hist(dAll$MVPA_min_per_day, breaks=1000, xlim=c(0,300))

#look at deciles
quantile(dAll$MVPA_min_per_day, prob = seq(0.1, 0.9, by = 0.1), na.rm = TRUE)
```
We can have a look at some of the machine-learned variables, to see if we believe them! Here we'll look at plots of probability of being in a particular behaviour by time of day:
```{r}
plotAverageDay(dAll, "sleep.hourOfDay.", ".avg", "Sleep (probability)")

```

Now let's look descriptively at something we might expect to vary as activity status varies: BMI.

```{r}
plotVarAndQuintile(dAll, # dataset
                   'acc.overall.avg', # exposure
                   'BMI', # outcome
                   FALSE # [TRUE/FALSE] save plots to PDF
                  )
```


You might want to extend this function to plot the different profiles of different groups (e.g. healthy/ unhealthy individuals, or age groups).

# Run a simple health association analysis
Let's run a minimally (age and sex) adjusted linear model for BMI, against fifths of average acceleration vector magnitude. This is attempting to model statistically the association we were looking at descriptively in the end of the last section.

```{r}
dAll$acc_quintiles <- cut(dAll$acc.overall.avg, breaks = c(quantile(dAll$acc.overall.avg, probs = seq(0, 1, by = 0.2), na.rm = TRUE))) # cut at quintiles
min_adj_lm_BMI <- lm(BMI ~acc_quintiles + broadAgeGroup+ sex, dAll)
```

We can look at the model summary:
```{r}
summary(min_adj_lm_BMI)
```

We can also look at the [model diagnostics](http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/) to understand more about the fit of the model: 
```{r}
plot(min_adj_lm_BMI)
```
The problem with the Q-Q plot is caused by the BMI distribution - log-transforming the outcome would be a good option in this case.  

# Next steps for modelling 

Here we've looked at a very simple linear model. Next steps might be:

- adjusting for confounders (e.g. perhaps socioeconomic status affects both level of activity and BMI)
- refining the definition of the exposure or looking at different exposure variables
- looking at different outcomes
- logistic regression for prevalent disease (use the `glm` function)
- Cox regression for incident disease (see for example the `survival` package and the `coxph` function)


# Getting started with Cox regression

For example, here's an initial Cox regression analysis associating quintiles of overall acceleration with incident ischaemic heart disease.

First we need to set up the data in an appropriate format, with a follow-up time and an indicator at exit indicating whether the participant exited due to an event or due to being censored (either the participant died of another cause or study data ended before they had an event).
```{r}
# Exclude participants with prevalent IHD
cat(sum(dAll$ischaemic.heart.disease.prevalent), " were excluded due to prior ischaemic heart disease")
dAll <- dAll[dAll$ischaemic.heart.disease.prevalent == 0, ]

# We need to process participants who died as they are censored earlier
# Replace the file location below with "/cdtshared/wearables/health_data_files/death.txt"
death <- read.csv("../data_and_data_prep/death.txt",
           sep = "\t")
death <- death[death$ins_index == 0, ]  #Keep just one record per participant (a very small number of participants have duplicate records)
dAll$died <- 0
dAll$died[dAll$eid %in% death$eid ] <- 1
dAll <- merge(dAll[, colnames(dAll)[colnames(dAll) != "date_of_death"]], death[, c("eid", "date_of_death")], by = "eid", all.x = TRUE)
cat(sum(dAll$died), "participants died")

# Note censoring dates of 31.12.20 in England/Scotland and 28.02.2018 in Wales (https://biobank.ndph.ox.ac.uk/ukb/exinfo.cgi?src=Data_providers_and_dates) 
dAll$censoring <- as.Date("31/12/2020", format = "%d/%m/%Y")
dAll$censoring[dAll$UkBiobankAssessCent %in% c("Cardiff", "Wrexham", "Swansea")] <- as.Date("28/02/2018", format = "%d/%m/%Y")

# For people who died, we need to censor them at the earliest of their date of death and overall censoring (e.g. a participant in Wales who died in 2020 should nonetheless be # censored at 28.02.2018 - if they had an IHD event in 2019 we wouldn't know about it
dAll$censoring[(dAll$died == 1)] <- pmin(dAll$censoring[dAll$died == 1], as.Date(dAll$date_of_death[dAll$died ==1], format = "%d/%m/%Y"))

# Add follow up variable (censor date for participants without an event, event date for participants with an event)
dAll$follow_up <- dAll$censoring
dAll$follow_up[(dAll$ischaemic.heart.disease.incident == 1)] <-  pmin(as.Date(dAll$ischaemic.heart.disease[(dAll$ischaemic.heart.disease.incident == 1)], format = "%Y-%m-%d"), dAll$censoring[(dAll$ischaemic.heart.disease.incident == 1)])

# Note event status at censoring (again, care taken as there are some instances of entries in the data after censoring)
dAll$IHD_at_exit <- 0
dAll$IHD_at_exit[(dAll$ischaemic.heart.disease.incident == 1) & (as.Date(dAll$ischaemic.heart.disease, format = "%Y-%m-%d") == dAll$follow_up)] <- 1
cat(sum(dAll$IHD_at_exit), " had incident hospital diagnosed ischaemic heart disease within the follow up period")

# Calculate follow up time 
dAll$fu_time <- as.numeric(difftime(dAll$follow_up, as.Date(dAll$EndTimWear, "%Y-%m-%d %H:%M:%S", tz = "Europe/London")))
```
WARNING: when working with date data, especially when dates are only present for some participants, it is very easy to write code which behaves in strange ways... I did so several times when writing this example (and don't guarantee it's error-free now). It is well worth inspecting your data repeatedly to check that the code is doing what you expect. [Obviously not shown here as I can't print the data frame on the internet... Bear this in mind when using RMarkdown]. 

We now have an event status indicator at exit and a follow-up time variable, which is enough to run a Cox model: 
```{r}
library(survival)
cox_model <- coxph(Surv(fu_time, IHD_at_exit) ~ broadAgeGroup + sex + acc_quintiles, dAll)
summary(cox_model)
```
The `exp(coef)` column gives the hazard ratio. Not surprisingly, older age and male sex are associated with higher risk of ischaemic heart disease, whereas a higher level of activity is associated with a lower risk of ischaemic heart disease. Again, you would probably want to substantially refine this model. For example, it would be good to adjust for possible confounders, and you might also want to look into model diagnostics to understand if assumptions, such as the [proportional hazards assumption](https://thomaselove.github.io/432-notes/cox-regression-models-for-survival-data-example-1.html) (see section 23.2.5 and 23.2.6). You might want to refine the outcome definition - e.g. a different set of ICD-10 codes, or adding in ischaemic heart disease events recorded only on the death register (by using information in `death_cause.txt`).  

A bonus: here we've adjusted the model for baseline age (in fact, in very crude groups) and used time-on-study as the timescale in the Cox regression analysis. This is what most introductory texts do. However, in epidemiological studies, time-on-study might not be the most relevant timescale. [Age might be a more appropriate timescale...](https://journals.lww.com/epidem/Fulltext/2012/07000/Proportional_Hazards_Regression_in_Epidemiologic.9.aspx)


Have fun! :) 

